{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Module_Diagnostics.ipynb\n",
      "testing compute_RMS(...):\n",
      "This should be close to 1: 0.9888686227526993\n",
      "This should be close to 0: 0.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pylab as pl\n",
    "import matplotlib\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rcParams\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from matplotlib.gridspec    import GridSpec\n",
    "import import_ipynb\n",
    "\n",
    "import Module_Diagnostics as MD\n",
    "import numpy as np\n",
    "from tempfile import mkdtemp\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "#Debugger. For some reason, using it inside a function works well. Otherwise...\n",
    "from IPython.core.debugger import set_trace\n",
    "#exemple: \n",
    "# def debug():\n",
    "#     set_trace()\n",
    "    \n",
    "#     `code_to_debug`\n",
    "    \n",
    "#     return\n",
    "\n",
    "# debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "rcParams[\"figure.figsize\"] = [9.4, 4.8]\n",
    "# matplotlib.use('nbagg') #_comment this line if you don't need to interact with plots (zoom, translations, savings...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose run and time for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date of the simulation (DD_MM_YY): 18_08_20\n"
     ]
    }
   ],
   "source": [
    "run_name = 'RUN_NAME'\n",
    "\n",
    "### Only if working on lx-moissard\n",
    "Cluster = 'Occ/'\n",
    "run_name = '20_08_18_new_big_one_0'\n",
    "filepath = '/data/Lathys/Visualisation/' + Cluster + run_name + '/ncfiles/'\n",
    "\n",
    "#This is used by the functions find_ip_shock(N, V) and find_mc_leading_edge(B)\n",
    "metadata = {'t_shock_entrance' : 130,\n",
    "            't_shock_exit'     : 240,\n",
    "            't_MC_entrance'    : 130,\n",
    "            't_MC_exit'        : 270}\n",
    "#todo: autodefine t_collision? maybe from story_reader will be easier, as lines will cross on the multivariate plot\n",
    "\n",
    "from_time = 210\n",
    "to_time = 220 #metadata['t_shock_exit']\n",
    "\n",
    "date = re.search('Magw_(.+?)_t', glob.glob(filepath+'Magw*_t'+ '%05d' % from_time +'.nc')[0]).group(1) \n",
    "\n",
    "print(f'date of the simulation (DD_MM_YY): {date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plt.savefig\n",
    "storing_directory = filepath + \"../shock_tracking/\"\n",
    "path_png = Path(storing_directory)\n",
    "if path_png.exists():\n",
    "    pass\n",
    "else:\n",
    "    path_png.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_directory_json = filepath + \"../json_files/\"\n",
    "\n",
    "path_store_json = Path(storing_directory_json)\n",
    "\n",
    "if not(path_store_json.exists()):\n",
    "    os.system(f'mkdir {path_store_json}')\n",
    "\n",
    "name = \"shock_tracking_bis_\" + run_name + \".json\"\n",
    "path_json = Path(storing_directory_json + name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data in Hsw, Magw and Elew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_slices(data, Hsw):\n",
    "    \n",
    "    x = np.array(np.around(Hsw['x']))\n",
    "    y = np.array(np.around(Hsw['y']))\n",
    "    z = np.array(np.around(Hsw['z']))\n",
    "\n",
    "    nx,  ny,  nz  = len(x), len(y), len(z)\n",
    "    # Location of the planet is defined in the .ncfiles as (x,y,z) = (0,0,0)\n",
    "    # Location of the planet is defined in the .ncfiles as (x,y,z) = (0,0,0)\n",
    "    nx0, ny0, nz0 = ( int(np.where(abs(x)==min(abs(x)))[0]),\n",
    "                      int(np.where(abs(y)==min(abs(y)))[0]), \n",
    "                      int(np.where(abs(z)==min(abs(z)))[0])  )\n",
    "    \n",
    "    result = {}\n",
    "        \n",
    "    for item in data:\n",
    "        \n",
    "        list_xy = {}\n",
    "        list_xz = {}\n",
    "        print(item)\n",
    "        \n",
    "        list_relevant_y = [-300, -100, -90, -80, -70, 0, 70, 80, 90, 100, 300]\n",
    "        list_iy = [np.where(y == r_y)[0][0] for r_y in list_relevant_y]\n",
    "        for iy in list_iy:\n",
    "            iy = iy\n",
    "            list_xy.update({'y = ' + str(y[iy]): [float(value) for value in ( Hsw[item][:, iy-1 , nz0]\n",
    "                                                                             +Hsw[item][:, iy   , nz0]\n",
    "                                                                             +Hsw[item][:, iy+1 , nz0] )/3] })\n",
    "        for iz in range(0, nz):            \n",
    "            list_xz.update({'z = ' + str(z[iz]): [float(value) for value in Hsw[item][:, ny0, iz ]] })\n",
    "        \n",
    "        print(f'mean value for {item} is {1./2*(np.nanmean(list(list_xy.values())) + np.nanmean(list(list_xz.values())))}')\n",
    "        result.update({  item: { '(xy) plane': list_xy,\n",
    "                                 '(xz) plane': list_xz }  })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -634.756823263642\n",
      "Vy\n",
      "mean value for Vy is -1.9863538455301883\n",
      "Vz\n",
      "mean value for Vz is 17.17109572449486\n",
      "There was no stored data. \n",
      "A '.json' containing a dict will be created. \n",
      "This dict will only contain the data for time 00210.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -636.2781859388406\n",
      "Vy\n",
      "mean value for Vy is -1.8538038125395442\n",
      "Vz\n",
      "mean value for Vz is 17.20272231926486\n",
      "This time_dump 00211 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -637.6793589681828\n",
      "Vy\n",
      "mean value for Vy is -1.9963906202910406\n",
      "Vz\n",
      "mean value for Vz is 17.285472303683996\n",
      "This time_dump 00212 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -639.2120849376827\n",
      "Vy\n",
      "mean value for Vy is -1.9453471353381047\n",
      "Vz\n",
      "mean value for Vz is 17.462640572615094\n",
      "This time_dump 00213 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -640.7978803475654\n",
      "Vy\n",
      "mean value for Vy is -1.9649430223118172\n",
      "Vz\n",
      "mean value for Vz is 17.566677926874\n",
      "This time_dump 00214 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -642.2948448922266\n",
      "Vy\n",
      "mean value for Vy is -2.0531300610914736\n",
      "Vz\n",
      "mean value for Vz is 17.6577650943194\n",
      "This time_dump 00215 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -643.8823110686416\n",
      "Vy\n",
      "mean value for Vy is -1.9310848069136028\n",
      "Vz\n",
      "mean value for Vz is 17.82788370208192\n",
      "This time_dump 00216 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -645.3913165380737\n",
      "Vy\n",
      "mean value for Vy is -2.023948950089599\n",
      "Vz\n",
      "mean value for Vz is 17.978895340776138\n",
      "This time_dump 00217 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -646.8959957950326\n",
      "Vy\n",
      "mean value for Vy is -1.9775996809964227\n",
      "Vz\n",
      "mean value for Vz is 18.068725425816424\n",
      "This time_dump 00218 did not have any data yet. Values were added.\n",
      "Writing new values\n",
      "Importing Hsw 3D from /data/Lathys/Visualisation/Occ/20_08_18_new_big_one_0/ncfiles/\n",
      "Reading density...\n",
      "Reading Ux...\n",
      "Reading Uy...\n",
      "Reading Uz...\n",
      "Reading T...\n",
      "Close file and return...\n",
      "Vx\n",
      "mean value for Vx is -648.4572093327588\n",
      "Vy\n",
      "mean value for Vy is -2.073556170301806\n",
      "Vz\n",
      "mean value for Vz is 18.17719239250111\n",
      "This time_dump 00219 did not have any data yet. Values were added.\n",
      "Writing new values\n"
     ]
    }
   ],
   "source": [
    "for time in range(from_time, to_time):\n",
    "    time = '%05d' % time    # Change the time to string format, needed by functions\n",
    "\n",
    "    ## Load Vxyz\n",
    "    Hsw = MD.import_data_3D(filepath, date, time, 'Hsw')\n",
    "  \n",
    "    x = np.array(np.around(Hsw['x']))\n",
    "    y = np.array(np.around(Hsw['y']))\n",
    "    z = np.array(np.around(Hsw['z']))\n",
    "\n",
    "    cwp = Hsw['c_omegapi']\n",
    "    gstep = Hsw['gstep']\n",
    "\n",
    "    nx,  ny,  nz  = len(x), len(y), len(z)\n",
    "    # Location of the planet is defined in the .ncfiles as (x,y,z) = (0,0,0)\n",
    "    # Location of the planet is defined in the .ncfiles as (x,y,z) = (0,0,0)\n",
    "    nx0, ny0, nz0 = (  int(np.where(abs(x)==min(abs(x)))[0]),\n",
    "                       int(np.where(abs(y)==min(abs(y)))[0]), \n",
    "                       int(np.where(abs(z)==min(abs(z)))[0])  )\n",
    "\n",
    "      \n",
    "    new_data = {  time: collect_slices(['Vx', 'Vy', 'Vz'], Hsw), 'x': [float(xj) for xj in x],\n",
    "                                                                 'y': [float(yj) for yj in y],\n",
    "                                                                 'z': [float(zj) for zj in z]  }\n",
    "\n",
    "    if path_json.exists():\n",
    "        with open(path_json, \"r\", encoding='utf-8') as shock_tracking:\n",
    "            stored_data = json.load(shock_tracking)\n",
    "\n",
    "            if (type(stored_data) == dict):\n",
    "                if time in stored_data:\n",
    "                    print(f\"Some values were already stored for this time dump {time}, they were updated.\")\n",
    "                    stored_data[time].update(new_data[time])\n",
    "                if time not in stored_data:\n",
    "                    print(f\"This time_dump {time} did not have any data yet. Values were added.\")\n",
    "                    stored_data.update(new_data)\n",
    "    else:\n",
    "        print(\"There was no stored data. \\n\"\n",
    "              \"A '.json' containing a dict will be created. \\n\"\n",
    "              f\"This dict will only contain the data for time {time}.\")\n",
    "        stored_data = new_data            \n",
    "\n",
    "    with open(path_json, \"w\", encoding='utf-8') as updated_shock_tracking:\n",
    "        print(\"Writing new values\")\n",
    "        json.dump(stored_data, updated_shock_tracking)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_shock_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
